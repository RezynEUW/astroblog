[["Map",1,2,9,10],"meta::meta",["Map",3,4,5,6,7,8],"astro-version","5.11.0","content-config-digest","e4ec1cd9de68d35c","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"site\":\"https://yourusername.netlify.app\",\"compressHTML\":true,\"base\":\"/\",\"trailingSlash\":\"ignore\",\"output\":\"static\",\"scopedStyleStrategy\":\"attribute\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":true,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":false,\"port\":4321,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{},\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[],\"remotePatterns\":[],\"responsiveStyles\":false},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":\"shiki\",\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"github-dark\",\"themes\":{},\"wrap\":true,\"transformers\":[]},\"remarkPlugins\":[],\"rehypePlugins\":[],\"remarkRehype\":{},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true},\"env\":{\"schema\":{},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"headingIdCompat\":false,\"preserveScriptOrder\":false,\"liveContentCollections\":false,\"csp\":false},\"legacy\":{\"collections\":false}}","blog",["Map",11,12,26,27,40,41],"gradient-descent",{"id":11,"data":13,"body":21,"filePath":22,"digest":23,"legacyId":24,"deferredRender":25},{"title":14,"description":15,"pubDate":16,"tags":17},"Understanding Gradient Descent","A visual guide to optimization in machine learning",["Date","2025-01-15T00:00:00.000Z"],[18,19,20],"machine-learning","optimization","algorithms","import ImageGallery from '../../components/ImageGallery.astro';\r\n\r\nGradient descent is fundamental to training machine learning models. Let's explore how it works.\r\n\r\n## The Basics\r\n\r\nGradient descent is an optimization algorithm used to minimize a cost function by iteratively moving in the direction of steepest descent.\r\n\r\nHere's a simple implementation in Python:\r\n\r\n```python\r\nimport numpy as np\r\n\r\ndef gradient_descent(X, y, learning_rate=0.01, iterations=1000):\r\n    m = len(y)\r\n    theta = np.zeros(X.shape[1])\r\n    \r\n    for _ in range(iterations):\r\n        predictions = X @ theta\r\n        errors = predictions - y\r\n        gradient = (1/m) * X.T @ errors\r\n        theta -= learning_rate * gradient\r\n    \r\n    return theta\r\n```\r\n\r\n## Mathematical Foundation\r\n\r\nThe update rule for gradient descent is:\r\n\r\n$$\\theta_{t+1} = \\theta_t - \\alpha \\nabla_\\theta J(\\theta)$$\r\n\r\nWhere:\r\n- $\\alpha$ is the learning rate\r\n- $J(\\theta)$ is our cost function\r\n- $\\nabla_\\theta J(\\theta)$ is the gradient\r\n\r\n## Visualization Example\r\n\r\n\u003CImageGallery \r\n  images={[\r\n    {\r\n      src: \"/images/placeholder1.jpg\",\r\n      alt: \"2D visualization of gradient descent\",\r\n      caption: \"Gradient descent finding the minimum\"\r\n    },\r\n    {\r\n      src: \"/images/placeholder1.jpg\",\r\n      alt: \"3D loss surface\",\r\n      caption: \"The loss landscape in 3D\"\r\n    }\r\n  ]}\r\n/>\r\n\r\n## Types of Gradient Descent\r\n\r\n1. **Batch Gradient Descent**: Uses entire dataset\r\n2. **Stochastic Gradient Descent**: Uses one sample at a time\r\n3. **Mini-batch Gradient Descent**: Uses small batches\r\n\r\nEach has trade-offs between computational efficiency and convergence stability.","src/content/blog/gradient-descent.mdx","518dd433bdafbaa5","gradient-descent.mdx",true,"how-to-write-posts",{"id":26,"data":28,"body":36,"filePath":37,"digest":38,"legacyId":39,"deferredRender":25},{"title":29,"description":30,"pubDate":31,"tags":32},"How to Write Posts","A guide to creating MDX posts with code, math, and images",["Date","2025-07-09T00:00:00.000Z"],[33,34,35],"guide","mdx","writing","import ImageGallery from '../../components/ImageGallery.astro';\r\n\r\nThis guide explains how to write blog posts for this site using MDX format.\r\n\r\n## File Structure\r\n\r\nCreate new posts in `src/content/blog/` with the filename format: `your-post-title.mdx`\r\n\r\n## Frontmatter\r\n\r\nEvery post must start with frontmatter:\r\n\r\n```yaml\r\n---\r\ntitle: 'Your Post Title'\r\ndescription: 'A brief description for previews'\r\npubDate: 2025-07-09\r\ntags: ['tag1', 'tag2', 'tag3']\r\n---\r\n```\r\n\r\n## Basic Formatting\r\n\r\nUse standard Markdown syntax:\r\n\r\n- **Bold text**: `**bold**`\r\n- *Italic text*: `*italic*`\r\n- `Inline code`: `` `code` ``\r\n- [Links](https://example.com): `[text](url)`\r\n\r\n## Code Blocks\r\n\r\nUse triple backticks with language specification:\r\n\r\n````markdown\r\n```python\r\ndef example():\r\n    return \"Hello, World!\"\r\n```\r\n````\r\n\r\nSupported languages: `python`, `javascript`, `typescript`, `bash`, `json`, `yaml`, and more.\r\n\r\n## Mathematical Equations\r\n\r\nUse LaTeX syntax for math:\r\n\r\nInline math: `$\\alpha + \\beta = \\gamma$` renders as $\\alpha + \\beta = \\gamma$\r\n\r\nBlock math:\r\n```markdown\r\n$$\r\n\\frac{\\partial L}{\\partial w} = \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) x^{(i)}\r\n$$\r\n```\r\n\r\nRenders as:\r\n\r\n$$\r\n\\frac{\\partial L}{\\partial w} = \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) x^{(i)}\r\n$$\r\n\r\n## Images and Galleries\r\n\r\nImport the ImageGallery component at the top of your post:\r\n\r\n```markdown\r\nimport ImageGallery from '../../components/ImageGallery.astro';\r\n```\r\n\r\nThen use it in your post:\r\n\r\n```jsx\r\n\u003CImageGallery \r\n  images={[\r\n    {\r\n      src: \"/images/example1.jpg\",\r\n      alt: \"Description of image\",\r\n      caption: \"Optional caption\"\r\n    },\r\n    {\r\n      src: \"/images/example2.jpg\",\r\n      alt: \"Another image\",\r\n      caption: \"Another caption\"\r\n    }\r\n  ]}\r\n  columns={2}\r\n/>\r\n```\r\n\r\n\u003CImageGallery \r\n  images={[\r\n    {\r\n      src: \"/images/placeholder1.jpg\",\r\n      alt: \"A picture of a bunny\",\r\n      caption: \"What a great dog\"\r\n    }\r\n  ]}\r\n/>\r\n\r\n\r\nPlace images in `public/images/` folder.\r\n\r\n## Lists\r\n\r\nOrdered lists:\r\n1. First item\r\n2. Second item\r\n3. Third item\r\n\r\nUnordered lists:\r\n- First item\r\n- Second item\r\n  - Nested item\r\n  - Another nested item\r\n\r\n## Tables\r\n\r\n```markdown\r\n| Algorithm | Time Complexity | Space Complexity |\r\n|-----------|----------------|------------------|\r\n| Bubble Sort | O(n²) | O(1) |\r\n| Quick Sort | O(n log n) | O(log n) |\r\n| Merge Sort | O(n log n) | O(n) |\r\n```\r\n\r\nRenders as:\r\n\r\n| Algorithm | Time Complexity | Space Complexity |\r\n|-----------|----------------|------------------|\r\n| Bubble Sort | O(n²) | O(1) |\r\n| Quick Sort | O(n log n) | O(log n) |\r\n| Merge Sort | O(n log n) | O(n) |\r\n\r\n\r\n## Publishing\r\n\r\n1. Save your `.mdx` file in `src/content/blog/`\r\n2. Commit and push to GitHub\r\n3. Netlify will automatically deploy\r\n\r\nYour post URL will be: `/blog/your-file-name`","src/content/blog/how-to-write-posts.mdx","2f07e8294cdb3456","how-to-write-posts.mdx","regularization-techniques-deep-dive",{"id":40,"data":42,"body":49,"filePath":50,"digest":51,"legacyId":52,"deferredRender":25},{"title":43,"description":44,"pubDate":45,"tags":46},"Mastering Regularization in Machine Learning","A deep dive into L1, L2, and dropout regularization techniques for robust model training.",["Date","2025-03-10T00:00:00.000Z"],[18,47,48],"regularization","stability","import ImageGallery from '../../components/ImageGallery.astro';\r\n\r\nRegularization is a fundamental set of techniques in machine learning that improve model generalization by discouraging overly complex or overfitted solutions. When a model learns both the underlying pattern and noise in the training data, it performs poorly on new, unseen data. Regularization methods add constraints or penalties during training to prevent this overfitting.\r\n\r\n### L2 Regularization (Ridge)\r\n\r\nL2 regularization adds a penalty proportional to the square of the magnitude of model weights. The modified cost function for linear regression becomes:\r\n\r\n$$\r\nJ(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m (h_{\\theta}(x^{(i)}) - y^{(i)})^2 + \\frac{\\lambda}{2m} \\sum_{j=1}^n \\theta_j^2\r\n$$\r\n\r\nWhere:\r\n- $\\lambda$ controls the strength of the penalty\r\n- Larger $\\lambda$ shrinks weights towards zero, smoothing the model\r\n\r\n```python\r\n# L2 Regularized Linear Regression using scikit-learn\r\nfrom sklearn.linear_model import Ridge\r\n\r\nX_train, X_test, y_train, y_test = ...  # your data split\r\nmodel = Ridge(alpha=1.0)  # alpha = lambda\r\nmodel.fit(X_train, y_train)\r\nprint(\"Test R²:\", model.score(X_test, y_test))\r\n```\r\n\r\n### L1 Regularization (Lasso)\r\n\r\nL1 regularization adds a penalty proportional to the absolute value of weights. This can drive some parameters to exactly zero, performing feature selection as well as regularization:\r\n\r\n$$\r\nJ(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m (h_{\\theta}(x^{(i)}) - y^{(i)})^2 + \\frac{\\lambda}{m} \\sum_{j=1}^n |\\theta_j|\r\n$$\r\n\r\n### Dropout in Neural Networks\r\n\r\nDropout randomly sets a fraction $p$ of neuron outputs to zero during each training batch, preventing co-adaptation of features. At inference, outputs are scaled by $(1-p)$:\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import layers, models\r\n\r\nmodel = models.Sequential([\r\n    layers.Dense(128, activation='relu', input_shape=(input_dim,)),\r\n    layers.Dropout(0.5),\r\n    layers.Dense(64, activation='relu'),\r\n    layers.Dropout(0.5),\r\n    layers.Dense(num_classes, activation='softmax')\r\n])\r\n```\r\n\r\n\u003CImageGallery\r\n  images={[\r\n    {\r\n      src: '/images/placeholder2.jpg',\r\n      alt: 'Comparison of decision boundaries with L2 regularization',\r\n      caption: 'L2 shrinks coefficients evenly'\r\n    },\r\n    {\r\n      src: '/images/placeholder2.jpg',\r\n      alt: 'Coefficient sparsity via L1 regularization',\r\n      caption: 'L1 zeros out less important features'\r\n    }\r\n  ]}\r\n/>\r\n\r\n### Choosing the Right Method\r\n\r\n- **L2** is good for large feature sets where you want to keep all features but penalize extreme weights.  \r\n- **L1** is ideal when you suspect many features are irrelevant — it performs automatic feature selection.  \r\n- **Dropout** is essential in deep neural networks to avoid interdependent feature activations.\r\n\r\nProper use of regularization often involves grid search or cross-validation to tune $\\lambda$ or dropout rate. By understanding these techniques and their trade-offs, you’ll be able to build more robust, generalizable models across a variety of tasks.","src/content/blog/regularization-techniques-deep-dive.mdx","d989568cd9a273ba","regularization-techniques-deep-dive.mdx"]